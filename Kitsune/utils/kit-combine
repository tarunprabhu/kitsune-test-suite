#!/usr/bin/env python3

import argparse as ap
import json
import os
import math
import re
import sys

# The regular expression to extract the name of the test and the tapir target
# from the name of the test script file. The name of the test is the first
# match, the tapir target is the third.
re_kit_test = re.compile('^(.+?)-(c|cxx|fortran|kokkos|unknown)-(.+?)[.]test$')

# The result expression to extract the name of the test and the language from
# name of the test script file. This name of the language is guaranteed to be
# either culang or hiplang.
re_vanilla_test = re.compile(
    '^(.+)-(nvcc|hipcc|kokkos-nvidia|kokkos-amd)[.]test$'
)

# Get the test name. The argument is a path to a .test file. In the case of a
# Benchmark or MultiSource test, the test name will be the name of the directory
# containing the .test file. For tests in the SingleSource directory, the base
# name will be of the form <test-name>-kit-<tapir-target>.test. Everything in
# the Kitsune/SingleSource directory is guaranteed to be a Kitsune test. Some
# tests in the MultiSource directory may have the same name as those in the
# SingleSource or Benchmark directories. In this case, tag them with a multifile
# suffix to ensure that we know what they are. We care about those because they
# use LTO, so we would like them to be distinct.
def get_name(base: str) -> str:
    if base.startswith('Kitsune/Benchmarks'):
        return os.path.basename(os.path.dirname(base))
    elif base.startswith('Kitsune/MultiSource'):
        return f'{os.path.basename(os.path.dirname(base))}-multifile'
    elif base.startswith('Kitsune/SingleSource'):
        b = os.path.basename(base)
        m = re_kit_test.match(b)
        if not m:
            sys.exit(f'Unexpected single-source test name: {b}')
        return m[1]
    else:
        sys.exit(f'Unexpected prefix in test base name: {base}')

# Get the tapir target from test name. The argument is a path to a .test file.
# If the test is a "Kokkos" test i.e. one where we recognize Kokkos constructs
# and compile them differently, the tapir target will have Kokkos prepended to
# it. Yes, it's not great, but the alternatives are even more complicated.
def get_tapir_target(base: str) -> str:
    b = os.path.basename(base)
    m = re_kit_test.match(b)
    if not m:
        sys.exit(f'Could not get tapir target: {base}')
    if m[2] == 'kokkos':
        return 'kokkos-' + m[3]
    return m[3]

# Get either the tapir target or the language name from the test name. The
# argument is the path to a .test file. Some of the benchmarks are straight cuda
# or hip source files that are not compiled with Kitsune and do not have a tapir
# target associated with them. In these cases, the name of the language will be
# part of the test name.
def get_tapir_target_or_lang(base: str) -> str:
    b = os.path.basename(base)
    m = re_vanilla_test.match(b)
    if not m:
        return get_tapir_target(base)
    elif m[2] in ['nvcc', 'hipcc', 'kokkos-nvidia', 'kokkos-amd']:
        return m[2]
    else:
        sys.exit(f'Could not get tapir target or language: {base}')

# Get the directory containing the object and timing files for a test with the
# given base name.
def get_objdir(build_dir: str, base: str) -> str:
    d = os.path.dirname(os.path.join(build_dir, base))
    b = re.sub('[.]test$', '', os.path.basename(base))

    return os.path.join(d, 'CMakeFiles', b + '.dir')

# Get the total number of object files found for a test.
def get_num_object_files(build_dir: str, base: str) -> int:
    objdir = get_objdir(build_dir, base)
    return len([f for f in os.listdir(objdir) if f.endswith('.o.time')])

# Get the total compile time for all object files that are found. This is not
# likely to be the wall-clock time observed by the user since the object files
# could have been compiled in parallel.
def get_compile_time(build_dir: str, base: str) -> int:
    objdir = get_objdir(build_dir, base)
    time = 0.0
    files = [f for f in os.listdir(objdir) if f.endswith('.o.time')]
    if files:
        for filename in files:
            with open(os.path.join(objdir, filename), 'r') as f:
                for line in f:
                    if line.startswith('real'):
                        time += float(line.split(' ')[-1].strip())
        # The time is recorded in seconds. But the per-kernel timings are in
        # microseconds. To keep things consistent, report this in microseconds
        return math.ceil(time * 1000000)
    else:
        return -1

# Get the link time.
def get_link_time(build_dir: str, base: str) -> int:
    d = os.path.dirname(os.path.join(build_dir, base))
    name = re.sub('[.]test$', '', os.path.basename(base))
    filename = os.path.join(d, name + '.link.time')

    # In benchmark mode, we run the vanilla cuda and hip code with nvcc and
    # hipcc respectively. Presumably because these are not the LLVM compilers
    # being tested, the test suite does not record link times.
    if not os.path.exists(filename):
        return -1

    time = 0.0
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith('real'):
                time = float(line.split(' ')[-1].strip())
    # The time is recorded in seconds. But the per-kernel timings are in
    # microseconds. To keep things consistent, report this in microseconds
    return math.ceil(time * 1000000)

# Process the output file from a benchmark test and add it to the given data
# object.
def parse_output_into(build_dir: str, base: str, data) -> None:
    test_file = os.path.join(build_dir, base)
    odir = os.path.join(os.path.dirname(test_file), 'Output')
    ofile = os.path.join(odir, os.path.basename(test_file) + '.out')
    lines = []
    with open(ofile, 'r') as f:
        record = False
        for line in f:
            if line == '<json>\n':
                record = True
            elif line == '</json>\n':
                record = False
            elif record:
                lines.append(line.strip())
    if lines:
        data['times'] = json.loads('\n'.join(lines))

# Process the report file and generate the combined report.
def process(report_file: str, build_dir: str):
    combined = {}

    # If the test-suite was run with the provided run-kitsune-tests script, the
    # CPU and GPU information on the test platform will have been saved to a
    # file named platform.json alongside the report.json file. This is optional,
    # so use it if it is found, but ignore it otherwise.
    parent = os.path.dirname(report_file)
    platform = os.path.join(parent, 'platform.json')
    if os.path.exists(platform):
        with open(platform, 'r') as f:
            combined = json.load(f)

    # We cannot have comments in a JSON file, but having notes in it is useful.
    combined['notes'] = [
        '1. All times are in microseconds',
        '2. Executable sizes are in bytes',
        '3. If a compile/link time could not be determined, it is -1',
        '4. For now, we cannot get compile and link times when using '
        'nvcc/hipcc. Neither can we collect the number of object files'
    ]

    combined['targets'] = []
    combined['tests'] = {}

    tts = set([])
    j = None
    with open(report_file, 'r') as f:
        j = json.load(f)

    for test in j['tests']:
        # The name of the test suite always starts with "test-suite :: "
        base = test['name'].replace('test-suite :: ', '')
        name = get_name(base)
        if name not in combined['tests']:
            combined['tests'][name] = {}

        tt = get_tapir_target_or_lang(base)
        tts.add(tt)

        data = {}

        # We can't use the compile and link times from the main report because
        # the values there are determined by looking for timing files that have
        # a prefix that matches the test name.
        data['test'] = base
        data['code'] = test['code'].lower()
        if data['code'] == 'pass':
            data['object-files'] = get_num_object_files(build_dir, base)
            data['compile-time'] = get_compile_time(build_dir, base)
            data['link-time'] = get_link_time(build_dir, base)
            data['size'] = test['metrics']['size']

            if base.startswith('Kitsune/Benchmarks'):
                parse_output_into(build_dir, base, data)

        combined['tests'][name][tt] = data

    combined['targets'] = sorted(tts)
    return combined

# Sanity check the command line arguments. Returns nothing if successful,
# terminates the program with an error otherwise.
def sanity_check(args):
    if not os.path.exists(args.report):
        sys.exit(f'Could not find report: {args.report}')

    # Make a reasonable attempt at checking the build directory.
    build_dir = args.build_dir
    if not build_dir:
        build_dir = os.path.dirname(args.report)

    if not os.path.exists(build_dir):
        sys.exit(f'Could not find build directory: {build_dir}')

    if not os.path.isdir(build_dir):
        sys.exit(f'Path is not a directory: {build_dir}')

# Parse the command line arguments. Returns the parsed object if parsing was
# successful. Will terminate the program with an error message otherwise.
def parse_command_line_args():
    prog = 'kit-combine'
    descr = (
        'Combine Kitsune test suite reports. When the test suite is run, the'
        'the "interesting" reports are spread across several files. This will'
        'generate a combined report, in JSON, with data collected from the '
        'various files. Not all of the data from the various files will be '
        'included in the result'
    )

    parser = ap.ArgumentParser(prog = prog, description = descr)
    parser.add_argument(
        '-o',
        '--output',
        type = str,
        default = None,
        dest = 'outfile',
        metavar = '<file>',
        help = 'Write the combined report to a file instead of stdout'
    )
    parser.add_argument(
        '-b',
        '--build-dir',
        type = str,
        default = None,
        dest = 'build_dir',
        metavar = '<dir>',
        help =
        'Path to the root of the build directory where the test suite is built '
        'and run'
    )
    parser.add_argument(
        'report',
        type = str,
        help =
        'Path to the "top-level" report produced when running the test-suite. '
        'This is assumed to be present in the Kitsune subdirectory of the '
        'top-level test-suite build directory. If it is not, the -b option '
        'should be provided'
    )

    return parser.parse_args()

def main():
    args = parse_command_line_args()
    sanity_check(args)

    build_dir = args.build_dir
    if not build_dir:
        # args.report will be in ${test_suite}/Kitsune. We want build_dir to
        # be ${test_suite}
        build_dir = os.path.dirname(os.path.dirname(args.report))

    combined = process(args.report, build_dir)
    pretty = json.dumps(combined, sort_keys = False, indent = 2)
    if args.outfile:
        with open(args.outfile, 'w') as f:
            f.write(pretty)
            f.write('\n')
    else:
        print(pretty)

if __name__ == '__main__':
    exit(main())
