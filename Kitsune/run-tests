#!/usr/bin/env sh
#
# The main reason this exists is to pass -j 1 to lit in order to run the tests.
# By default, lit runs tests in parallel, but we cannot do that. Some CPU tapir
# targets like OpenCilk will use all the cores on the system by default. Running
# tests in parallel under such conditions is usually undesirable. Similarly,
# running the tests with GPU tapir targets in parallel will cause contention on
# the GPU which is not ideal either. It may be ok on a multi-gpu system, but for
# now, we don't check for that. The user can always use lit directly in those
# cases.
#
# There might be some way of passing options to lit via a config file, but I
# haven't found it if it exists. This script cannot be used on Windows, but we
# don't support Windows in Kitsune anyway.
#
# This script has since expanded to also record the hardware details on the
# system where this was run and collating interesting compile and execution
# time statistics. Some of these are reports from the tests themselves.

usage="run-tests [OPTIONS]
Try run-tests -h for more information"

help="
Run the Kitsune tests. This passes the right options to lit, records information
about the platform on which the tests are run (number and type of GPU's, CPU
core count etc) and combines useful compile and execution time statistics into
a single, more concise, report.

USAGE

    run-tests [OPTIONS]

OPTIONS

    -h          Show help and exit

    -o <file>   Where to write the combined report. If this is not given, it
                will be written to report.json in the Kistune/ subdirectory of
                the test suite

    -b <dir>    The Kitsune build directory. This is useful if we strictly
                need the llvm-lit executable that is present there. Otherwise,
                the lit executable from $PATH will be used
"

# This will be the Kitsune/ subdirectory within the root of the llvm-test-suite
# build directory
kitsune=$(dirname $0)

# The root of the llvm-test-suite build directory.
test_suite=$(dirname "${kitsune}")

build_dir=""
outfile="${kitsune}/report.json"

while getopts "b:ho:" opt; do
	case "${opt}" in
	b)
		build_dir="${OPTARG}"
		;;
	h)
		echo "${help}"
		exit 0
		;;
	o)
		outfile="${OPTARG}"
		;;
	*)
		echo "${usage}"
		exit 1
		;;
	esac
done

lit=
if [ -n "${build_dir}" ]; then
	lit=${build_dir}/bin/llvm-lit
else
	lit=$(command -v lit)
fi

if [ -z "${lit}" ] || ! [ -x "${lit}" ]; then
	echo "Could not find suitable lit executable"
	exit 1
fi

# TODO: It is not clear that we need NO_STOP_MESSAGE=1 for the Fortran tests,
# but keep it for now. We will definitely want to revisit this at some point.
export NO_STOP_MESSAGE=1
${lit} -s -j 1 -o ${kitsune}/ts-report.json ${kitsune}

# This should return a non-zero exit code if any tests failed.
pass=$?

# It is unlikely that there will be CPU's with different models on the test
# node. But if that does happen, we don't really care since this is only
# intended to be informational.
cpu="<<UNKNOWN>>"
cores="0"
if ls -l /proc/cpuinfo >/dev/null 2>&1; then
	cpu=$(cat /proc/cpuinfo |
		grep "model name" |
		sed 's/model name\s*:\s*//g' |
		sort |
		uniq)
	cores=$(cat /proc/cpuinfo |
		grep "processor" |
		sed 's/processor\s*:\s*//g' |
		wc -l)
else
	echo "Looking up CPU info on non-Linux platforms is not implemented"
fi

nvidia_smi=$(command -v nvidia-smi)
[ $? -ne 0 ] && nvidia_smi=""

rocm_smi=$(command -v rocm-smi)
[ $? -ne 0 ] && rocm_smi=""

# There could be more than one GPU on the machine. It would be good to determine
# which one was actually used, but at the time of writing, we cannot. It is
# unlikely that the suite will be run on a node with a variety of different
# GPU's on it. The more likely case is that there are multiple devices of the
# same model. Optimize for that scenario but be flexible enough to record all
# the GPU's if that is not the case.
#
gpus=""
count=0
if [ -n "${nvidia_smi}" ]; then
	# If no GPU's are found, nvidia-smi -L will return an error code.
	report=$(nvidia-smi -L)
	if [ $? -eq 0 ]; then
		names=$(echo "${report}" |
			sed -E 's/GPU[ ]*[0-9]+:[ ]*//g' |
			sed -E 's/[ ]*[(]UUID:[ ]*.+[)]//g' |
			sort |
			uniq)
		n=$(echo "${names}" | wc -l)
		namelist=$(echo "${names}" |
			tr '\n' ';' |
			sed -E 's/;$//g')
		[ -n "${gpus}" ] && gpus="${gpus};"
		gpus="${gpus}${namelist}"
		count=$((count + n))
	fi
fi
if [ -n "${rocm_smi}" ]; then
	# rocm-smi will never return an error code, even if no GPU's were found.
	# Instead, stdout will be empty with errors in stderr.
	report=$(rocm-smi -i 2>/dev/null)
	if [ -n "${report}" ]; then
		names=$(rocm-smi -i |
			grep "Device Name" |
			awk -F':' '{print $3;}' |
			sed -E 's/^\s*//g' |
			sort |
			uniq)
		n=$(echo "${names}" | wc -l)
		namelist=$(echo "${names}" |
			tr '\n' ';' |
			sed -E 's/;$//g')
		[ -n "${gpus}" ] && gpus="${gpus};"
		gpus="${gpus}${namelist}"
		count=$((count + n))
	fi
fi

date=$(date --rfc-3339=seconds)
host=$(hostname)

# Determine the version of Kitsune used when building these tests. We cannot
# assume that it will be in $PATH, so look at the CMakeCache.txt in the root of
# the test suite build directory. This should always be available. Kitsune's C
# frontend should always be available.
version=""
kitcc=$(grep "C_COMPILER:" "${test_suite}/CMakeCache.txt" |
	grep -oE "=.+$" |
	tr -d '=')
if [ -n "${kitcc}" ]; then
	version=$(${kitcc} --version |
		head -n 1 |
		grep -oE "version.+$" |
		sed 's/version[ ]*//g')
fi

platform="{
  \"date\": \"${date}\",
  \"version\": \"${version}\",
  \"hostname\": \"${host}\",
  \"cpu\": {
    \"model\": \"${cpu}\",
    \"cores\": ${cores}
  },
  \"gpu\": {
    \"count\": ${count},
    \"devices\": \"${gpus}\"
  }
}"

echo "${platform}" >${kitsune}/platform.json

if [ ${pass} -eq 0 ]; then
	# Generate the combined report. There is no reason to do this as a separate
	# step. It can always be done separately if needed. The -b option must be
	# given the root of the test suite.
	${kitsune}/utils/kit-combine \
		-b "${test_suite}" \
		-o "${outfile}" \
		"${kitsune}/ts-report.json"
fi

exit ${pass}
